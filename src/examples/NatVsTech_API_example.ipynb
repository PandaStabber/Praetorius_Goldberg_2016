{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports \n",
    "Note: python3. Please install requirements using requirments.txt in main directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import fnmatch\n",
    "import os.path\n",
    "import pandas as pd\n",
    "# import API one directory above\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"),os.path.pardir)))\n",
    "from NatVsTech import NatVsTech\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Directory structure\n",
    "Note: we generically define directory so it will work on any OS: mac/pc/linux.\n",
    "Note: drop the \"\" around \"__file__\" when in a regular python file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tron/Praetorius_Goldberg_2016\n",
      "/Users/tron/Praetorius_Goldberg_2016/examples/databases\n",
      "/Users/tron/Praetorius_Goldberg_2016/examples/databases/training_data\n",
      "/Users/tron/Praetorius_Goldberg_2016/examples/output\n"
     ]
    }
   ],
   "source": [
    "PARENT_PATH = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"),os.path.pardir))\n",
    "DATABASES_BASEPATH = os.path.abspath(os.path.join(os.path.dirname(\"__file__\"),'databases'))\n",
    "IMPORT_TRAINING_DATABASE_PATH = os.path.abspath(\n",
    "    os.path.join(DATABASES_BASEPATH, 'training_data'))\n",
    "IMPORT_TESTING_DATABASE_PATH = os.path.abspath(\n",
    "    os.path.join(DATABASES_BASEPATH,'test_data'))\n",
    "OUTPUT_DATA_SUMMARY_PATH = os.path.abspath(\n",
    "    os.path.join(os.path.dirname(\"__file__\"), 'output'))\n",
    "\n",
    "\n",
    "# print the paths, just to make sure things make sense\n",
    "print(PARENT_PATH)\n",
    "print(DATABASES_BASEPATH)\n",
    "print(IMPORT_TRAINING_DATABASE_PATH)\n",
    "print (OUTPUT_DATA_SUMMARY_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training files\n",
    "Import training files, combine, and concatenate into dataframes. \n",
    "Note: if you re-run the notebook without resetting the kernal, you'll get an error. Restart the notebook kernal and it will work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural_training_data.csv', 'technical_training_data.csv']\n"
     ]
    }
   ],
   "source": [
    "# set the natural and technical database training file names\n",
    "NATURAL_TRAINING_DATABASE_NAME_ = 'natural_training_data.csv'\n",
    "TECHNICAL_TRAINING_DATABASE_NAME_ = 'technical_training_data.csv'\n",
    "\n",
    "# change the directory to the import training data path\n",
    "os.chdir(IMPORT_TRAINING_DATABASE_PATH)\n",
    "\n",
    "# find all csv's in the directory\n",
    "training_files = glob.glob('*.csv')\n",
    "\n",
    "# iterate through files and assign classification id\n",
    "for file in training_files:\n",
    "    if fnmatch.fnmatchcase(file, TECHNICAL_TRAINING_DATABASE_NAME_):\n",
    "        technical_training_database = pd.DataFrame.from_csv(\n",
    "            os.path.join(file),header=0, index_col=None)\n",
    "        \n",
    "        # assign classification id\n",
    "        technical_training_database['classification'] = 0\n",
    "        \n",
    "    elif fnmatch.fnmatchcase(file, NATURAL_TRAINING_DATABASE_NAME_):\n",
    "        natural_training_database = pd.DataFrame.from_csv(\n",
    "            os.path.join(file), header=0, index_col=None)\n",
    "        \n",
    "        # assign classification id\n",
    "        natural_training_database['classification'] = 1\n",
    "\n",
    "print (training_files)\n",
    "# concatenate all the data into a single file\n",
    "training_data = pd.concat([natural_training_database, \n",
    "                           technical_training_database])\n",
    "\n",
    "# remoove all the na values (other filtering done later)\n",
    "training_data = training_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using the API\n",
    "Before you can use the API, you have to initialize the class. We'll then work through how the data is easily filtered, stored, and used for training and prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NatVsTech.NatVsTech object at 0x10bab5400>\n"
     ]
    }
   ],
   "source": [
    "# initialize class\n",
    "nat_v_tech = NatVsTech()\n",
    "\n",
    "print (nat_v_tech)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     25Mg    55Mn    59Co    60Ni   65Cu    66Zn    88Sr    90Zr    93Nb  \\\n",
      "0  2.6333  0.0000  0.0000  0.7809  0.000  1.6047  6.2237  0.0000  0.0000   \n",
      "1  1.5857  1.3047  0.0000  0.7762  0.000  0.0000  1.2381  0.0000  1.1667   \n",
      "2  0.0000  0.0000  0.0000  0.6381  1.719  0.0000  0.0000  1.4714  0.0000   \n",
      "3  5.3095  1.2286  0.8476  0.0000  0.000  0.0000  2.6285  0.0000  0.0000   \n",
      "4  0.0000  1.2428  0.0000  0.0000  0.000  0.6095  2.2666  0.0000  0.0000   \n",
      "\n",
      "     95Mo       ...         149Sm   153Eu   157Gd  159Tb    182W   206Pb  \\\n",
      "0  0.0000       ...        0.0000  0.0000  0.0000    0.0  0.7095  1.0524   \n",
      "1  1.3143       ...        0.0000  0.1667  0.0000    0.0  0.9571  0.5333   \n",
      "2  1.7619       ...        0.0000  0.0000  0.3476    0.0  0.1095  0.4333   \n",
      "3  0.0000       ...        1.3952  0.0000  0.0000    0.0  0.4524  1.3809   \n",
      "4  0.0000       ...        0.0000  1.0000  0.0000    0.0  0.0000  1.0714   \n",
      "\n",
      "    208Pb   232Th    238U  classification  \n",
      "0  2.3904  1.0809  0.0000               1  \n",
      "1  1.9476  0.0000  0.0095               1  \n",
      "2  0.5571  1.6238  0.2143               1  \n",
      "3  3.2762  0.1810  0.0000               1  \n",
      "4  1.3190  0.0000  0.0000               1  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# filter the data of negative values\n",
    "neg_filt_training_data = nat_v_tech.filter_negative(data=training_data)\n",
    "\n",
    "# threshold the data with a single isotope trigger\n",
    "thresh_neg_filt_training_data = nat_v_tech.apply_detection_threshold(\n",
    "                                    data=neg_filt_training_data, \n",
    "                                    threshold_value=5)\n",
    "\n",
    "# print to maake sure we're on target\n",
    "print (thresh_neg_filt_training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     25Mg    55Mn    59Co    60Ni   65Cu    66Zn    88Sr    90Zr    93Nb  \\\n",
      "0  2.6333  0.0000  0.0000  0.7809  0.000  1.6047  6.2237  0.0000  0.0000   \n",
      "1  1.5857  1.3047  0.0000  0.7762  0.000  0.0000  1.2381  0.0000  1.1667   \n",
      "2  0.0000  0.0000  0.0000  0.6381  1.719  0.0000  0.0000  1.4714  0.0000   \n",
      "3  5.3095  1.2286  0.8476  0.0000  0.000  0.0000  2.6285  0.0000  0.0000   \n",
      "4  0.0000  1.2428  0.0000  0.0000  0.000  0.6095  2.2666  0.0000  0.0000   \n",
      "\n",
      "     95Mo   ...     147Sm   149Sm   153Eu   157Gd  159Tb    182W   206Pb  \\\n",
      "0  0.0000   ...    0.0000  0.0000  0.0000  0.0000    0.0  0.7095  1.0524   \n",
      "1  1.3143   ...    1.0667  0.0000  0.1667  0.0000    0.0  0.9571  0.5333   \n",
      "2  1.7619   ...    0.0000  0.0000  0.0000  0.3476    0.0  0.1095  0.4333   \n",
      "3  0.0000   ...    0.0000  1.3952  0.0000  0.0000    0.0  0.4524  1.3809   \n",
      "4  0.0000   ...    0.0000  0.0000  1.0000  0.0000    0.0  0.0000  1.0714   \n",
      "\n",
      "    208Pb   232Th    238U  \n",
      "0  2.3904  1.0809  0.0000  \n",
      "1  1.9476  0.0000  0.0095  \n",
      "2  0.5571  1.6238  0.2143  \n",
      "3  3.2762  0.1810  0.0000  \n",
      "4  1.3190  0.0000  0.0000  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: classification, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# right now training data contains the classification data. Split it. \n",
    "(training_df, target_df) = nat_v_tech.split_target_from_training_data(\n",
    "                                df=thresh_neg_filt_training_data)\n",
    "\n",
    "\n",
    "# print training data to check structure\n",
    "print (training_df.head())\n",
    "\n",
    "# print target data to check structure\n",
    "print (target_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_features': 'sqrt', 'learning_rate': 0.1, 'min_samples_leaf': 100, 'n_estimators': 1000, 'loss': 'deviance', 'random_state': None, 'max_depth': 5}\n",
      "{'learning_rate': [0.01, 0.1], 'min_samples_leaf': [50, 100], 'max_features': ['sqrt', 'log2'], 'loss': ['exponential', 'deviance'], 'n_estimators': [100], 'random_state': [None], 'max_depth': [5]}\n",
      "GradientBoostingClassifierrWithCoef(criterion='friedman_mse', init=None,\n",
      "                  learning_rate=0.1, loss='deviance', max_depth=5,\n",
      "                  max_features='sqrt', max_leaf_nodes=None,\n",
      "                  min_impurity_split=1e-07, min_samples_leaf=100,\n",
      "                  min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "                  n_estimators=1000, presort='auto', random_state=None,\n",
      "                  subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5072e84884e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                                    \u001b[0mcv_grid_search\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGBC_GRID_SEARCH_PARAMS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                                    \u001b[0mgbc_init_params\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mGBC_INIT_PARAMS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                                    n_splits = 3)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tron/Praetorius_Goldberg_2016/NatVsTech.py\u001b[0m in \u001b[0;36mrfecv_feature_identify\u001b[0;34m(self, training_df, target_df, n_splits, test_size, random_state, cv_grid_search, gbc_init_params, kfolds, gbc_grid_params, find_min_boosting_stages)\u001b[0m\n\u001b[1;32m    269\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcv_grid_search\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m \t\t\t\tgbc_grid_fitted_params = self.find_optimum_gbc_parameters(crossfolds=5,\n\u001b[0m\u001b[1;32m    272\u001b[0m                                                                                                                                                   \u001b[0mtraining_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                                                                                                                                                   \u001b[0mtarget_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/tron/Praetorius_Goldberg_2016/NatVsTech.py\u001b[0m in \u001b[0;36mfind_optimum_gbc_parameters\u001b[0;34m(self, crossfolds, training_df, target_df, gbc_search_params)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;31m# call the grid search fit using the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0mgrid_searcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;31m# store and print the best parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             (train_scores, test_scores, test_sample_counts,\n\u001b[0;32m--> 570\u001b[0;31m              fit_time, score_time, parameters) = zip(*out)\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             (test_scores, test_sample_counts,\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 0)"
     ]
    }
   ],
   "source": [
    "# conform the test data for ML and store it as X and y.\n",
    "# (X, y) = nat_v_tech.conform_data_for_ML(training_df=training_df, target_df=target_df)\n",
    "\n",
    "# initialize gbc parameters to determine max estimators with least overfitting\n",
    "GBC_INIT_PARAMS = {'loss': 'deviance', 'learning_rate': 0.1,\n",
    "\t\t\t\t   'min_samples_leaf': 100, 'n_estimators': 1000,\n",
    "\t\t\t\t   'max_depth': 5, 'random_state': None, 'max_features': 'sqrt'}\n",
    "\n",
    "# print to verify parameter init structure\n",
    "print (GBC_INIT_PARAMS)\n",
    "\n",
    "# outline grid search parameters\n",
    "# set optimum boosting stages. Note: n_estimators automatically set\n",
    "GBC_GRID_SEARCH_PARAMS = {'loss': ['exponential', 'deviance'],\n",
    "\t\t\t\t\t\t  'learning_rate': [0.01, 0.1],\n",
    "\t\t\t\t\t\t  'min_samples_leaf': [50, 100],\n",
    "\t\t\t\t\t\t  'random_state': [None],\n",
    "\t\t\t\t\t\t  'max_features': ['sqrt', 'log2'],\n",
    "\t\t\t\t\t\t  'max_depth': [5],\n",
    "\t\t\t\t\t\t  'n_estimators': [100]}  \n",
    "\n",
    "print (GBC_GRID_SEARCH_PARAMS)\n",
    "\n",
    "# determining optimum feature selection with rfecv \n",
    "nat_v_tech.rfecv_feature_identify(training_df = training_df,target_df = target_df,\n",
    "                                   cv_grid_search=GBC_GRID_SEARCH_PARAMS, \n",
    "                                   gbc_init_params =GBC_INIT_PARAMS,\n",
    "                                   n_splits = 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_state': None, 'learning_rate': 0.1, 'loss': 'deviance', 'max_features': 'sqrt', 'n_estimators': 1000, 'min_samples_leaf': 100, 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "410\n"
     ]
    }
   ],
   "source": [
    "# find optimum boosting stages\n",
    "optimum_boosting_stages = nat_v_tech.find_min_boosting_stages(gbc_base_params=GBC_INIT_PARAMS,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  training_df=training_df,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  target_df=target_df)[1]\n",
    "\n",
    "# print optimum boosting stages\n",
    "print (optimum_boosting_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'random_state': [None], 'learning_rate': [0.01, 0.1], 'loss': ['exponential', 'deviance'], 'max_features': ['sqrt', 'log2'], 'n_estimators': [410], 'min_samples_leaf': [50, 100], 'max_depth': [5]}\n"
     ]
    }
   ],
   "source": [
    "# create grid search parameters in which to find the optimum set, \n",
    "# set optimum boosting stages. Note: n_estimators automatically set\n",
    "GBC_GRID_SEARCH_PARAMS = {'loss': ['exponential', 'deviance'],\n",
    "\t\t\t\t\t\t  'learning_rate': [0.01, 0.1],\n",
    "\t\t\t\t\t\t  'min_samples_leaf': [50, 100],\n",
    "\t\t\t\t\t\t  'random_state': [None],\n",
    "\t\t\t\t\t\t  'max_features': ['sqrt', 'log2'],\n",
    "\t\t\t\t\t\t  'max_depth': [5],\n",
    "\t\t\t\t\t\t  'n_estimators': [optimum_boosting_stages]}  \n",
    "\n",
    "# print search parameter grid to verify init structure\n",
    "print (GBC_GRID_SEARCH_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='exponential', max_depth=5,\n",
      "              max_features='log2', max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=50,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=410, presort='auto', random_state=None,\n",
      "              subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# find the optimum gbc parameters\n",
    "gbc_fitted = nat_v_tech.find_optimum_gbc_parameters(crossfolds=5,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ttraining_df=training_df,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\ttarget_df=target_df,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tgbc_search_params=GBC_GRID_SEARCH_PARAMS)\n",
    "\n",
    "# print the optimum gbc structure\n",
    "print (gbc_fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
      "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
      "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 155, 156,\n",
      "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
      "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
      "       209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
      "       222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
      "       235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "       248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260,\n",
      "       261, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274,\n",
      "       275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287,\n",
      "       288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300,\n",
      "       301, 302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 314,\n",
      "       315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327,\n",
      "       328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340,\n",
      "       341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "       354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366,\n",
      "       367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379,\n",
      "       380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392,\n",
      "       393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "       406, 407, 408, 409, 410, 411, 412, 413, 415, 416, 417, 418, 419,\n",
      "       420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
      "       433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445,\n",
      "       446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458,\n",
      "       459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471,\n",
      "       472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484,\n",
      "       485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497,\n",
      "       498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510,\n",
      "       511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523,\n",
      "       524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536,\n",
      "       537, 538, 539, 540, 541, 542, 543, 544, 545]),)\n",
      "(array([  0,   1,   2,   3,   4,   5,   6,   7,   9,  10,  11,  12,  13,\n",
      "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
      "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
      "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
      "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
      "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
      "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
      "       105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118,\n",
      "       119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,\n",
      "       132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144,\n",
      "       145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
      "       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
      "       171, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184,\n",
      "       185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197,\n",
      "       198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210,\n",
      "       211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
      "       224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236,\n",
      "       237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249,\n",
      "       250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262,\n",
      "       263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275,\n",
      "       276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288,\n",
      "       289, 290, 291, 292, 293, 294, 295, 296, 298, 299, 300, 301, 302,\n",
      "       303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n",
      "       316, 317, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
      "       330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342,\n",
      "       343, 344, 345, 347, 348, 349, 351, 352, 353, 354, 355, 356, 357,\n",
      "       358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370,\n",
      "       371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383,\n",
      "       384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396,\n",
      "       397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409,\n",
      "       410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422,\n",
      "       423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435,\n",
      "       436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448,\n",
      "       449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461,\n",
      "       462, 463, 464, 465, 466, 467, 468, 469, 471, 472, 473, 474, 475,\n",
      "       476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488,\n",
      "       489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501,\n",
      "       502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514,\n",
      "       515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527,\n",
      "       528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540,\n",
      "       541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553,\n",
      "       554, 555, 556]),)\n",
      "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
      "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
      "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
      "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
      "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
      "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
      "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
      "       105, 106, 107, 108, 109, 110, 112, 113, 114, 116, 117, 118, 119,\n",
      "       120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132,\n",
      "       133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146,\n",
      "       147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159,\n",
      "       160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172,\n",
      "       173, 174, 175, 176, 177, 178, 179, 180, 182, 183, 184, 185, 186,\n",
      "       187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199,\n",
      "       200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
      "       213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
      "       226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
      "       239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
      "       252, 253, 254, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265,\n",
      "       266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278,\n",
      "       279, 280, 281, 282, 283, 284, 286, 287, 288, 289, 290, 292, 293,\n",
      "       296, 297, 298, 299, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
      "       310, 311, 312, 313, 314, 315, 317, 318, 319, 320, 321, 322, 323,\n",
      "       324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
      "       337, 338, 339, 340, 341, 344, 345, 346, 347, 348, 349, 350, 351,\n",
      "       352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
      "       365, 366, 367, 368, 369, 370, 371, 372, 374, 375, 376, 377, 378,\n",
      "       379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391,\n",
      "       392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
      "       405, 406, 407, 408, 409, 410, 411, 412, 414, 415, 416, 417, 419,\n",
      "       420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432,\n",
      "       433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 444, 445, 446,\n",
      "       447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459,\n",
      "       460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472,\n",
      "       473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485,\n",
      "       486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498,\n",
      "       499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       553, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566,\n",
      "       567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579,\n",
      "       580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592,\n",
      "       593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605,\n",
      "       607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 619, 620,\n",
      "       621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633,\n",
      "       634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 645, 646, 647,\n",
      "       648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660,\n",
      "       661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673,\n",
      "       674, 675, 676, 677, 678, 679, 681, 682, 683, 684, 685, 686, 687,\n",
      "       688, 689, 690, 691, 692, 694, 695, 696, 697, 698, 699, 700, 701,\n",
      "       702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n",
      "       715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 726, 727, 728,\n",
      "       729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 741, 742,\n",
      "       743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755,\n",
      "       756, 757, 758, 759, 760, 761, 762, 763, 764]),)\n",
      "(array([  1,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  15,\n",
      "        18,  19,  20,  22,  23,  24,  29,  30,  31,  32,  33,  34,  35,\n",
      "        36,  38,  40,  41,  44,  45,  46,  47,  49,  50,  51,  52,  53,\n",
      "        54,  55,  56,  57,  59,  60,  62,  64,  65,  66,  67,  69,  70,\n",
      "        71,  72,  75,  76,  77,  78,  79,  80,  81,  82,  84,  85,  86,\n",
      "        87,  89,  90,  91,  92,  93,  95,  97,  99, 100, 101, 102, 103,\n",
      "       104, 105, 106, 107, 109, 111, 112, 113, 114, 115, 117, 118, 119,\n",
      "       121, 122, 124, 125, 126, 127, 128, 131, 132, 133, 134, 135, 136,\n",
      "       137, 138, 139, 140, 141, 142, 143, 145, 146, 148, 149, 151, 152,\n",
      "       154, 155, 156, 158, 159, 160, 161, 164, 166, 167, 168, 169, 172,\n",
      "       173, 174, 176, 177, 178, 179, 181, 182, 183, 185, 188, 191, 193,\n",
      "       194, 196, 197, 198, 201, 203, 205, 206, 208, 209, 210, 211, 214,\n",
      "       215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227,\n",
      "       228, 229, 230, 231, 232, 233, 235, 236, 237, 239, 240, 241, 243,\n",
      "       244, 245, 246, 247, 248, 250, 251, 252, 253, 255, 256, 257, 258,\n",
      "       259, 260, 262, 265, 266, 267, 268, 269, 270, 271, 273, 274, 275,\n",
      "       276, 277, 278, 279, 280, 281, 282, 283, 284, 289, 291, 292, 294,\n",
      "       295, 297, 299, 300, 301, 302, 303, 304, 305, 307, 309, 310, 311,\n",
      "       313, 314, 315, 316, 317, 320, 322, 326, 327, 328, 331, 333, 334,\n",
      "       335, 336, 338, 339, 340, 341, 343, 344, 346, 347, 348, 352, 353,\n",
      "       354, 355, 357, 358, 359, 361, 362, 363, 364, 371, 372, 373, 374,\n",
      "       375, 376, 377, 378, 379, 382, 383, 384, 385, 387, 388, 389, 390,\n",
      "       391, 392, 393, 394, 397, 398, 399, 402, 405, 407, 408, 409, 410,\n",
      "       411, 412, 413, 414, 416, 417, 419, 420, 422, 425, 427, 428, 429,\n",
      "       430, 431, 432, 433, 434, 436, 437, 438, 440, 441, 442, 443, 444,\n",
      "       445, 446, 447, 448, 450, 452, 453, 454, 455, 456, 457, 459, 460,\n",
      "       462, 463, 464, 465, 466, 467, 468, 470, 471, 472, 473, 474, 475,\n",
      "       476, 477, 478, 479, 480, 481, 482, 484, 485, 486, 488, 490, 491,\n",
      "       492, 493, 494, 496, 497, 498, 500, 501, 502, 503, 504, 505, 506,\n",
      "       507, 508, 509, 510, 512, 513, 514, 515, 516, 518, 519, 520, 521,\n",
      "       523, 524, 525, 526, 527, 528, 530, 531, 532, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       553, 554, 555, 557, 558, 559, 562, 565, 566, 569, 570, 571, 572,\n",
      "       573, 574, 575, 576, 577, 579, 580, 582, 583, 584, 585, 586, 588,\n",
      "       589, 590, 591, 592, 593, 595, 596, 597, 599, 601, 602, 603, 604,\n",
      "       606, 607, 609, 610, 612, 615, 618, 620, 621, 622, 623, 625, 626,\n",
      "       627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 640, 641, 642,\n",
      "       643, 645, 648, 649, 652, 653, 654, 656, 657, 658, 659, 661, 662,\n",
      "       665, 666, 667, 668, 669, 670, 671, 672, 673, 675, 677, 680, 681,\n",
      "       682, 683, 684, 685, 687, 688, 689, 690, 691, 692, 693, 694, 695,\n",
      "       697, 698, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710,\n",
      "       711, 712, 716, 717, 718, 719, 720, 721, 722, 723, 725, 726, 728,\n",
      "       729, 730, 731, 733, 735, 736, 738, 739, 742, 743, 744, 746, 747,\n",
      "       749, 750, 752, 753, 754, 755, 756, 757, 758, 759, 761, 762, 763,\n",
      "       765, 766, 767, 768, 770, 771, 772, 774, 775, 776, 777, 779, 782,\n",
      "       783, 784, 786, 788, 790, 792, 796, 797, 799, 800, 801, 803, 805,\n",
      "       806, 808, 809, 810, 811, 812, 814, 815, 816, 817, 819, 821, 822,\n",
      "       824, 825, 826, 827, 830, 831, 832, 836, 837, 838, 839, 841, 842,\n",
      "       843, 844, 847, 848, 849, 850, 851, 852, 856, 858, 859, 860, 862,\n",
      "       863, 864, 865, 867, 868, 869, 872, 873, 874, 875, 876, 877, 881,\n",
      "       883, 884, 885, 886, 887, 888, 890, 895, 896, 897, 898, 900, 901,\n",
      "       904, 907, 908, 909, 911, 913, 914, 915, 916, 918, 919, 921, 922,\n",
      "       924, 925, 926, 927, 929, 930, 931, 932, 934, 936, 937, 938, 939,\n",
      "       940, 942, 943, 944, 945, 946]),)\n",
      "(array([   2,    3,    5, ..., 2895, 2896, 2897]),)\n",
      "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  85,  86,  87,  88,  89,  90,  91,\n",
      "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
      "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
      "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
      "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
      "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
      "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
      "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
      "       209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
      "       222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
      "       235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "       248, 249, 250, 251, 252, 253, 254, 255, 256, 258, 259, 260, 261,\n",
      "       262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274,\n",
      "       275, 276, 277, 278, 279, 280, 281, 282, 284, 285, 286, 287, 288,\n",
      "       289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301,\n",
      "       302, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 314, 315,\n",
      "       316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328,\n",
      "       329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341,\n",
      "       342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354,\n",
      "       355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "       368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381]),)\n",
      "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  11,  12,  13,\n",
      "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
      "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
      "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
      "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  66,\n",
      "        67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,\n",
      "        80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,\n",
      "        93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104, 105,\n",
      "       106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118,\n",
      "       119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131,\n",
      "       132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144,\n",
      "       145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
      "       158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
      "       171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
      "       184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196,\n",
      "       197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
      "       210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222,\n",
      "       223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235,\n",
      "       236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248,\n",
      "       249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261,\n",
      "       262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274,\n",
      "       275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287,\n",
      "       288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301,\n",
      "       302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314,\n",
      "       315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327,\n",
      "       328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340,\n",
      "       341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353,\n",
      "       354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366,\n",
      "       367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379,\n",
      "       380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392,\n",
      "       393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405,\n",
      "       406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418,\n",
      "       419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
      "       432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444,\n",
      "       445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457,\n",
      "       458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470,\n",
      "       471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483,\n",
      "       484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496,\n",
      "       497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509,\n",
      "       510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522,\n",
      "       523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535,\n",
      "       536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548,\n",
      "       549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561,\n",
      "       562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574,\n",
      "       575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587,\n",
      "       588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600,\n",
      "       601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613,\n",
      "       614, 615, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627,\n",
      "       628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640,\n",
      "       641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653,\n",
      "       654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666,\n",
      "       667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679,\n",
      "       680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692,\n",
      "       693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705,\n",
      "       706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 718, 719,\n",
      "       720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
      "       733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745,\n",
      "       746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758,\n",
      "       759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771,\n",
      "       772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784,\n",
      "       785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797,\n",
      "       798, 799, 800, 801, 802, 803, 804, 806, 807, 808, 809, 810, 811,\n",
      "       812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824,\n",
      "       825, 826, 827, 828, 829, 830, 831]),)\n",
      "(array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
      "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
      "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
      "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
      "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
      "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
      "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
      "        91,  92,  93,  94,  95,  96,  98,  99, 100, 101, 102, 103, 104,\n",
      "       105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117,\n",
      "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
      "       131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n",
      "       144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156,\n",
      "       157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169,\n",
      "       170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182,\n",
      "       183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
      "       196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208,\n",
      "       209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221,\n",
      "       222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234,\n",
      "       235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247,\n",
      "       248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260,\n",
      "       261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273,\n",
      "       274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286,\n",
      "       287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299,\n",
      "       300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312,\n",
      "       313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325,\n",
      "       326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338,\n",
      "       339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351,\n",
      "       352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364,\n",
      "       365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377,\n",
      "       378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390,\n",
      "       391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403,\n",
      "       404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416,\n",
      "       417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429,\n",
      "       430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442,\n",
      "       443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455,\n",
      "       456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468,\n",
      "       469, 470, 471, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482,\n",
      "       483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495,\n",
      "       496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508,\n",
      "       509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521,\n",
      "       522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 535,\n",
      "       536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548,\n",
      "       549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561,\n",
      "       562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574,\n",
      "       575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587,\n",
      "       588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600,\n",
      "       601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613,\n",
      "       614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626,\n",
      "       627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639,\n",
      "       640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652,\n",
      "       653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665,\n",
      "       666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678,\n",
      "       679, 680, 681, 682]),)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# use the X and y data to train the model. Then test the trained model against the test data and output results.\n",
    "nat_v_tech.apply_trained_classification(test_data_path=IMPORT_TESTING_DATABASE_PATH,\n",
    "\t\t\t\t\t\t\t\t\t\toutput_summary_data_path=OUTPUT_DATA_SUMMARY_PATH,\n",
    "\t\t\t\t\t\t\t\t\t\toutput_summary_base_name='summary.csv',\n",
    "\t\t\t\t\t\t\t\t\t\ttrack_class_probabilities=[0.1, 0.1],\n",
    "\t\t\t\t\t\t\t\t\t\tisotope_trigger='140Ce',\n",
    "\t\t\t\t\t\t\t\t\t\tgbc_fitted=gbc_fitted,\n",
    "\t\t\t\t\t\t\t\t\t\tX=X, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
